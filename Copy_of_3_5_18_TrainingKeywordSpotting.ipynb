{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaykay4403/embedded_sys/blob/main/Copy_of_3_5_18_TrainingKeywordSpotting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO4-CY_TCZZS"
      },
      "source": [
        "# Training Keyword Spotting\n",
        "This notebook builds on the Colab in which we used the pre-trained [micro_speech](https://github.com/tensorflow/tensorflow/tree/v2.4.1/tensorflow/lite/micro/examples/micro_speech) example.\n",
        "\n",
        "**It is now time for you to train your own custom keyword spotting model using the Speech Commands dataset!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0HQ7WkLUxD4"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtAfqQ9MGSiX"
      },
      "source": [
        "## Import packages\n",
        "Clone the TensorFlow Github Repository, which contains the relevant code required to run this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olCcGuF7GRVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e176aff9-7872-4fe1-dd2e-ccdd77a17748"
      },
      "source": [
        "!wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
        "!unzip v2.4.1.zip &> 0\n",
        "!mv tensorflow-2.4.1/ tensorflow/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 01:10:54--  https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/tensorflow/tensorflow/zip/refs/tags/v2.4.1 [following]\n",
            "--2023-04-05 01:10:54--  https://codeload.github.com/tensorflow/tensorflow/zip/refs/tags/v2.4.1\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘v2.4.1.zip.2’\n",
            "\n",
            "v2.4.1.zip.2            [   <=>              ]  66.13M  9.03MB/s    in 7.5s    \n",
            "\n",
            "2023-04-05 01:11:02 (8.83 MB/s) - ‘v2.4.1.zip.2’ saved [69346072]\n",
            "\n",
            "mv: cannot move 'tensorflow-2.4.1/' to 'tensorflow/tensorflow-2.4.1': Directory not empty\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important - Reverting to TF1 - Start\n",
        "We need to install Tensorflow V1 in order to use the required features for Keyword Spotting.\n",
        "\n",
        "**Run the following cell and click \"restart runtime\" that appears once it completes.**\n",
        "\n",
        "**Do not rerun the previous cells after restarting the runtime.**"
      ],
      "metadata": {
        "id": "STWW6rbX77X5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y\n",
        "#!pip install tensorflow-gpu==1.15\n",
        "!pip install tensorflow-gpu==2.12.0"
      ],
      "metadata": {
        "id": "iwp8q8_z61K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3574f4b7-dfc2-4df6-d41d-cbd0f72a2c39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-gpu==2.12.0\n",
            "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important - Reverting to TF1 - End\n",
        "\n",
        "**Make sure to click the \"RESTART RUNTIME\" button in the output of the previous cell. Then move onto the next cell!**"
      ],
      "metadata": {
        "id": "qNUJOOk68j1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/tensorflow/tensorflow/examples/speech_commands/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EJr2DYr_scT",
        "outputId": "5fe88b41-432f-412b-ecec-9be195ccc5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '/content/tensorflow/tensorflow/examples/speech_commands/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#assert(tf.VERSION == \"1.15.0\") #Make sure you install Tensorflow 1.15.0 and restart the runtime or this will fail!\n",
        "# Now that the runtime is set import things!\n",
        "import sys\n",
        "# We add this path so we can import the speech processing modules.\n",
        "sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
        "import input_data\n",
        "import models\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "-AQ4oR6v7HM0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "c8bda3b6-b296-4ba7-ba32-9396649951c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-af3bd875f2a5>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# We add this path so we can import the speech processing modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/tensorflow/tensorflow/examples/speech_commands/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tensorflow/tensorflow/examples/speech_commands/input_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_audio_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0maudio_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVtYN4nlCft"
      },
      "source": [
        "## Configure Your Model!\n",
        "Select your keywords and model settings with which to train!\n",
        "\n",
        "**This is where you need to make choices and input data!**\n",
        "\n",
        "```WANTED_WORDS``` = A comma-delimited string of the words you want to train for (e.g., \"yes,no\"). All the other words you do not select will be used to train an \"unknown\" label so that the model does not just recognize speech but your specific words. Audio data with no spoken words will be used to train a \"silence\" label. We suggest picking 2-4 words for best results.\n",
        "\n",
        "Options for target words are (PICK FROM THIS LIST FOR BEST RESULTS): \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\",  \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", “backward”, “forward”, “follow”, “learn”,\n",
        "\n",
        "Additional words that will be used to help train the \"unkown\" label are: \"bed\", \"bird\", \"cat\", \"dog\", \"happy\", \"house\", \"marvin\", \"sheila\", \"tree\", \"wow\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ludfxbNIaegy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b87fa5-5cab-4cfb-ffbf-73e03a0f6b7d"
      },
      "source": [
        "# A comma-delimited list\n",
        "WANTED_WORDS = \"up, down\"\n",
        "\n",
        "# Print the configuration to confirm it\n",
        "print(\"Spotting these words: %s\" % WANTED_WORDS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spotting these words: up, down\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8VBOW8Ob7ys"
      },
      "source": [
        "The number of training steps and learning rates can be specified as comma-separated strings to define the amount/rate at each stage. For example, ```TRAINING_STEPS=\"12000,3000\"``` and ```LEARNING_RATE=\"0.001,0.0001\"``` will run 12,000 training steps with a rate of 0.001 followed by 3,000 final steps with a learning rate of 0.0001. These are good default values to work off of when you choose your values as the course staff has gotten this to work well with those values in the past!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1eWOvdfcPlo"
      },
      "source": [
        "TRAINING_STEPS = \"12000, 3000\"\n",
        "\n",
        "LEARNING_RATE = \"0.001, 0.0001\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q36xx9vcQVp"
      },
      "source": [
        "We suggest you leave the ```MODEL_ARCHITECTURE``` as tiny_conv the first time but if you would like to do this again and explore additional models some options are: ```single_fc, conv, low_latency_conv, low_latency_svdf, tiny_embedding_conv```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Y1phllccMn"
      },
      "source": [
        "MODEL_ARCHITECTURE = 'tiny_conv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhj3jQr0cfCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3fa192-fa5c-4ebb-943c-156bbd3fc34d"
      },
      "source": [
        "# Calculate the total number of steps, which is used to identify the checkpoint\n",
        "# file name.\n",
        "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
        "\n",
        "# Print the configuration to confirm it\n",
        "print(\"Training these words: %s\" % WANTED_WORDS)\n",
        "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
        "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
        "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training these words: up, down\n",
            "Training steps in each stage: 12000, 3000\n",
            "Learning rate in each stage: 0.001, 0.0001\n",
            "Total number of training steps: 15000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCgeOpvY9pAi"
      },
      "source": [
        "**DO NOT MODIFY** the following constants as they include filepaths used in this notebook and data that is shared during training and inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd1iM1o2ymvA"
      },
      "source": [
        "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
        "# to ensure that we have equal number of samples for each label.\n",
        "number_of_labels = WANTED_WORDS.count(',') + 1\n",
        "number_of_total_labels = number_of_labels + 2 # for 'silence' and 'unknown' label\n",
        "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
        "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
        "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
        "\n",
        "# Constants which are shared during training and inference\n",
        "PREPROCESS = 'micro'\n",
        "WINDOW_STRIDE = 20\n",
        "\n",
        "# Constants used during training only\n",
        "VERBOSITY = 'DEBUG'\n",
        "EVAL_STEP_INTERVAL = '1000'\n",
        "SAVE_STEP_INTERVAL = '1000'\n",
        "\n",
        "# Constants for training directories and filepaths\n",
        "DATASET_DIR =  'dataset/'\n",
        "LOGS_DIR = 'logs/'\n",
        "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
        "\n",
        "# Constants for inference directories and filepaths\n",
        "import os\n",
        "MODELS_DIR = 'models'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "  os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
        "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
        "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
        "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
        "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
        "\n",
        "# Constants for Quantization\n",
        "QUANT_INPUT_MIN = 0.0\n",
        "QUANT_INPUT_MAX = 26.0\n",
        "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN\n",
        "\n",
        "# Constants for audio process during Quantization and Evaluation\n",
        "SAMPLE_RATE = 16000\n",
        "CLIP_DURATION_MS = 1000\n",
        "WINDOW_SIZE_MS = 30.0\n",
        "FEATURE_BIN_COUNT = 40\n",
        "BACKGROUND_FREQUENCY = 0.8\n",
        "BACKGROUND_VOLUME_RANGE = 0.1\n",
        "TIME_SHIFT_MS = 100.0\n",
        "\n",
        "# URL for the dataset and train/val/test split\n",
        "DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "VALIDATION_PERCENTAGE = 10\n",
        "TESTING_PERCENTAGE = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Calculate the correct flattened input data shape for later use in model conversion\n",
        "# since the model takes a flattened version of the spectrogram. The shape is number of\n",
        "# overlapping windows times the number of frequency bins. For the default settings we have\n",
        "# 40 bins (as set above) times 49 windows (as calculated below) so the shape is (1,1960)\n",
        "def window_counter(total_samples, window_size, stride):\n",
        "  '''helper function to count the number of full-length overlapping windows'''\n",
        "  window_count = 0\n",
        "  sample_index = 0\n",
        "  while True:\n",
        "    window = range(sample_index,sample_index+stride)\n",
        "    if window.stop < total_samples:\n",
        "      window_count += 1\n",
        "    else:\n",
        "      break\n",
        "\n",
        "    sample_index += stride\n",
        "  return window_count\n",
        "\n",
        "OVERLAPPING_WINDOWS = window_counter(CLIP_DURATION_MS, int(WINDOW_SIZE_MS), WINDOW_STRIDE)\n",
        "FLATTENED_SPECTROGRAM_SHAPE = (1, OVERLAPPING_WINDOWS * FEATURE_BIN_COUNT)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sw86LiFA6y80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UczQKtqLi7OJ"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gSank0ZVBK2"
      },
      "source": [
        "### Load in TensorBoard to visulaize the training process.\n",
        "\n",
        "As training progresses you should see the training status show up in the Tensorboard area. If this works it is very helpful for analyzing your training progress. Unfortunately, the staff has found that it sometimes doesn't start showing data for a while (~15 minutes) and sometimes doesn't show data until training completes (and instead shows ```No dashboards are active for the current data set```.). If it is working and then stops updating look to the top of the cell and click reconnect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozkJqgSkU_qI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "93e23f52-afe1-4a12-b0a0-d7abb49d5443"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOGS_DIR}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
              "contains an executable `tensorboard` program, or explicitly specify\n",
              "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
              "environment variable."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorboard\n",
        "!pip show tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq4gi13a4des",
        "outputId": "4f2828f5-6c43-488c-a6d9-74eff617aaed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: tensorboard\n",
            "Version: 2.12.0\n",
            "Summary: TensorBoard lets you watch Tensors Flow\n",
            "Home-page: https://github.com/tensorflow/tensorboard\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: absl-py, google-auth, google-auth-oauthlib, grpcio, markdown, numpy, protobuf, requests, setuptools, tensorboard-data-server, tensorboard-plugin-wit, werkzeug, wheel\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creates brower view of tensorboard\n",
        "# TensorBoard 2.12.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
        "#python /usr/local/lib/python3.9/dist-packages/tensorboard/main.py --logdir=./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co0OIN0y7X6r",
        "outputId": "b0a728a3-5a7d-4284-ab2a-4017d4e8402c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow installation not found - running with reduced feature set.\n",
            "/usr/local/lib/python3.9/dist-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.33' not found (required by /usr/local/lib/python3.9/dist-packages/tensorboard_data_server/bin/server)\n",
            "/usr/local/lib/python3.9/dist-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.34' not found (required by /usr/local/lib/python3.9/dist-packages/tensorboard_data_server/bin/server)\n",
            "/usr/local/lib/python3.9/dist-packages/tensorboard_data_server/bin/server: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.32' not found (required by /usr/local/lib/python3.9/dist-packages/tensorboard_data_server/bin/server)\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.12.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorboard/main.py\", line 53, in <module>\n",
            "    run_main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorboard/main.py\", line 46, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorboard/program.py\", line 276, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorboard/program.py\", line 295, in _run_serve_subcommand\n",
            "    server.serve_forever()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/werkzeug/serving.py\", line 770, in serve_forever\n",
            "    self.server_close()\n",
            "  File \"/usr/lib/python3.9/socketserver.py\", line 700, in server_close\n",
            "    super().server_close()\n",
            "  File \"/usr/lib/python3.9/socketserver.py\", line 483, in server_close\n",
            "    self.socket.close()\n",
            "  File \"/usr/lib/python3.9/socket.py\", line 501, in close\n",
            "    self._real_close()\n",
            "  File \"/usr/lib/python3.9/socket.py\", line 495, in _real_close\n",
            "    _ss.close(self)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gweRC9eWVdg2"
      },
      "source": [
        "### Launch Training\n",
        "\n",
        "**ARE YOU USING A GPU INSTANCE!?!?!**\n",
        "Make sure to check ```runtime->change runtime type``` as a GPU runtime will take ~2 hours to complete training whereas a CPU runtime with take ~10 hours!\n",
        "\n",
        "**Importantly, every hour or so you should make sure to remind colab you are still there by clicking around in a cell or clicking reconnect on the tensorboard cell (after 90 minutes of no activity Colab may kill the instance)!** But/and beyond that you can absolutely minimize the window and continue with other work.\n",
        "\n",
        "**If you run training a second time** we suggest restarting your runtime ```runtime->restart runtime``` in order to ensure all caches are cleared. However, do note that doing so will clear all cells and so you will have to run the entire file again.\n",
        "\n",
        "If you would like to get more information on the training script you can find the source code for the script [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/speech_commands/train.py). In short it sets up the optimizer and preprocessor based on all of the flags we pass in!\n",
        "\n",
        "Finally, by setting the ```VERBOSITY = 'DEBUG'``` above be aware that the training cell will print A LOT of information. Specifically you will get the accuracy and loss at each step as well as a confusion matrix every 1000 steps. We hope that is helpful in case TensorBoard fails to work. If you would like to run with less printouts you can change the setting to ```WARN``` or ```FATAL```. You will find this in the \"Configure Your Model!\" section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZw3VNlnla-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5d153c-5b8f-480a-dbc5-54d221f47320"
      },
      "source": [
        "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
        "--data_dir={DATASET_DIR} \\\n",
        "--wanted_words={WANTED_WORDS} \\\n",
        "--silence_percentage={SILENT_PERCENTAGE} \\\n",
        "--unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
        "--preprocess={PREPROCESS} \\\n",
        "--window_stride={WINDOW_STRIDE} \\\n",
        "--model_architecture={MODEL_ARCHITECTURE} \\\n",
        "--how_many_training_steps={TRAINING_STEPS} \\\n",
        "--learning_rate={LEARNING_RATE} \\\n",
        "--train_dir={TRAIN_DIR} \\\n",
        "--summaries_dir={LOGS_DIR} \\\n",
        "--verbosity={VERBOSITY} \\\n",
        "--eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
        "--save_step_interval={SAVE_STEP_INTERVAL}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/tensorflow/tensorflow/examples/speech_commands/train.py\", line 79, in <module>\n",
            "    import tensorflow as tf\n",
            "ModuleNotFoundError: No module named 'tensorflow'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjBn-NapWsho"
      },
      "source": [
        "# Generating your Model\n",
        "Just like with the pre-trained model we will now take the final checkpoint and convert it into a quantized TensorFlow Lite model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUJLrdS-ftl"
      },
      "source": [
        "### Generate a TensorFlow Model for Inference\n",
        "\n",
        "Combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as freezing a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyc3_eLh9sAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4735b95d-b2df-4e79-c90f-3e4c7ed4ec3a"
      },
      "source": [
        "!rm -rf {SAVED_MODEL}\n",
        "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
        "--wanted_words=$WANTED_WORDS \\\n",
        "--window_stride_ms=$WINDOW_STRIDE \\\n",
        "--preprocess=$PREPROCESS \\\n",
        "--model_architecture=$MODEL_ARCHITECTURE \\\n",
        "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
        "--save_format=saved_model \\\n",
        "--output_file={SAVED_MODEL}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/tensorflow/tensorflow/examples/speech_commands/freeze.py\", line 45, in <module>\n",
            "    import tensorflow as tf\n",
            "ModuleNotFoundError: No module named 'tensorflow'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DBGDxVI-nKG"
      },
      "source": [
        "### Generate a TensorFlow Lite Model\n",
        "\n",
        "Convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices.\n",
        "\n",
        "The following cell will also print the model size, which will be under 20 kilobytes.\n",
        "\n",
        "We download the dataset to use as a representative dataset for more thoughtful post training quantization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNQdAplJV1fz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "f2e04bec-0fcc-4a36-cf39-4787b45bb1ec"
      },
      "source": [
        "model_settings = models.prepare_model_settings(\n",
        "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
        "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "audio_processor = input_data.AudioProcessor(\n",
        "    DATA_URL, DATASET_DIR,\n",
        "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
        "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
        "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-582b489634f7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_settings = models.prepare_model_settings(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_words_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWANTED_WORDS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP_DURATION_MS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_SIZE_MS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n\u001b[1;32m      5\u001b[0m audio_processor = input_data.AudioProcessor(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBj_AyCh1cC0"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  # with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
        "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  float_tflite_model = float_converter.convert()\n",
        "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
        "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.inference_input_type = tf.lite.constants.INT8\n",
        "  # converter.inference_input_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x\n",
        "  converter.inference_output_type = tf.lite.constants.INT8\n",
        "  # converter.inference_output_type = tf.compat.v1.lite.constants.INT8 #replaces the above line for use with TF2.x\n",
        "  def representative_dataset_gen():\n",
        "    for i in range(100):\n",
        "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
        "                                         BACKGROUND_FREQUENCY,\n",
        "                                         BACKGROUND_VOLUME_RANGE,\n",
        "                                         TIME_SHIFT_MS,\n",
        "                                         'testing',\n",
        "                                         sess)\n",
        "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(FLATTENED_SPECTROGRAM_SHAPE)\n",
        "      yield [flattened_data]\n",
        "  converter.representative_dataset = representative_dataset_gen\n",
        "  tflite_model = converter.convert()\n",
        "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
        "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeLiDZTbLkzv"
      },
      "source": [
        "### Testing the accuracy after Quantization\n",
        "\n",
        "Verify that the model we've exported is still accurate, using the TF Lite Python API and our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQsEteKRLryJ"
      },
      "source": [
        "# Helper function to run inference\n",
        "def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n",
        "  #\n",
        "  # Load test data\n",
        "  #\n",
        "  np.random.seed(0) # set random seed for reproducible test results.\n",
        "  with tf.Session() as sess:\n",
        "  # with tf.compat.v1.Session() as sess: #replaces the above line for use with TF2.x\n",
        "    test_data, test_labels = audio_processor.get_data(\n",
        "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
        "        TIME_SHIFT_MS, 'testing', sess)\n",
        "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
        "\n",
        "  #\n",
        "  # Initialize the interpreter\n",
        "  #\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  #\n",
        "  # For quantized models, manually quantize the input data from float to integer\n",
        "  #\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "  #\n",
        "  # Evaluate the predictions\n",
        "  #\n",
        "  correct_predictions = 0\n",
        "  for i in range(len(test_data)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    top_prediction = output.argmax()\n",
        "    correct_predictions += (top_prediction == test_labels[i])\n",
        "\n",
        "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
        "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-pD52Na6jRa"
      },
      "source": [
        "# Compute float model accuracy\n",
        "run_tflite_inference_testSet(FLOAT_MODEL_TFLITE)\n",
        "\n",
        "# Compute quantized model accuracy\n",
        "run_tflite_inference_testSet(MODEL_TFLITE, model_type='Quantized')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDdZHRuFLPdH"
      },
      "source": [
        "# Testing the model with your own data!\n",
        "Now comes the fun part. It's time to test your model with your own realworld data. We'll proceed in the same way we tested the pre-trained model. Have fun!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VrEGTt5Pt1e"
      },
      "source": [
        "### Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoTsiK2Xtf3s"
      },
      "source": [
        "!pip install ffmpeg-python &> 0\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "!pip install librosa\n",
        "import librosa\n",
        "import scipy.io.wavfile\n",
        "!git clone https://github.com/petewarden/extract_loudest_section.git\n",
        "!make -C extract_loudest_section/\n",
        "print(\"Packages Imported, Extract_Loudest_Section Built\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKXSI_YJXUkw"
      },
      "source": [
        "### Define the helper function to run inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuz390pSPxcG"
      },
      "source": [
        "# Helper function to run inference (on a single input this time)\n",
        "# Note: this also includes additional manual pre-processing\n",
        "TF_SESS = tf.compat.v1.InteractiveSession()\n",
        "def run_tflite_inference_singleFile(tflite_model_path, custom_audio, sr_custom_audio, model_type=\"Float\"):\n",
        "  #\n",
        "  # Preprocess the sample to get the features we pass to the model\n",
        "  #\n",
        "  # First re-sample to the needed rate (and convert to mono if needed)\n",
        "  custom_audio_resampled = librosa.resample(librosa.to_mono(np.float64(custom_audio)), sr_custom_audio, SAMPLE_RATE)\n",
        "  # Then extract the loudest one second\n",
        "  scipy.io.wavfile.write('custom_audio.wav', SAMPLE_RATE, np.int16(custom_audio_resampled))\n",
        "  !/tmp/extract_loudest_section/gen/bin/extract_loudest_section custom_audio.wav ./trimmed\n",
        "  # Finally pass it through the TFLiteMicro preprocessor to produce the\n",
        "  # spectrogram/MFCC input that the model expects\n",
        "  custom_model_settings = models.prepare_model_settings(\n",
        "      0, SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "      WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "  custom_audio_processor = input_data.AudioProcessor(None, None, 0, 0, '', 0, 0,\n",
        "                                                    model_settings, None)\n",
        "  custom_audio_preprocessed = custom_audio_processor.get_features_for_wav(\n",
        "                                        'trimmed/custom_audio.wav', model_settings, TF_SESS)\n",
        "  # Reshape the output into a 1,1960 matrix as that is what the model expects\n",
        "  custom_audio_input = custom_audio_preprocessed[0].flatten()\n",
        "  test_data = np.reshape(custom_audio_input,(1,len(custom_audio_input)))\n",
        "\n",
        "  #\n",
        "  # Initialize the interpreter\n",
        "  #\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  #\n",
        "  # For quantized models, manually quantize the input data from float to integer\n",
        "  #\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "  #\n",
        "  # Run the interpreter\n",
        "  #\n",
        "  interpreter.set_tensor(input_details[\"index\"], test_data)\n",
        "  interpreter.invoke()\n",
        "  output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "  top_prediction = output.argmax()\n",
        "\n",
        "  #\n",
        "  # Translate the output\n",
        "  #\n",
        "  top_prediction_str = ''\n",
        "  if top_prediction >= 2:\n",
        "    top_prediction_str = WANTED_WORDS.split(',')[top_prediction-2]\n",
        "  elif top_prediction == 0:\n",
        "    top_prediction_str = 'silence'\n",
        "  else:\n",
        "    top_prediction_str = 'unknown'\n",
        "\n",
        "  print('%s model guessed the value to be %s' % (model_type, top_prediction_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41DVwrZBM8Jf"
      },
      "source": [
        "### Define the audio importing function\n",
        "Adapted from: https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/ and https://colab.research.google.com/drive/1Z6VIRZ_sX314hyev3Gm5gBqvm1wQVo-a#scrollTo=RtMcXr3o6gxN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy8gUzGtM5FK"
      },
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav', ac='1')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr\n",
        "print(\"Chrome Audio Recorder Defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aLBVlkeNC8B"
      },
      "source": [
        "### Record your own audio and test the model!\n",
        "After you run the record cell wait for the stop button to appear then start recording and then press the button to stop the recording once you have said the word!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYaZxXE0M_C9"
      },
      "source": [
        "custom_audio, sr_custom_audio = get_audio()\n",
        "print(\"DONE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PloZwcVhpZuY"
      },
      "source": [
        "# Then test the model\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, custom_audio, sr_custom_audio, model_type=\"Quantized\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}